{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures for Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents:\n",
    "\n",
    "- B-Trees for range searches of numerical and date values-\n",
    "- Inverted Index for searching relevant documents based on text.\n",
    "- Doc Values (Columnar data representatios) for fast field/column-wise aggregation operations. \n",
    "- KD-Trees for nearest neighbor searches with low-dimensional vectors.\n",
    "\n",
    "<!--\n",
    "- HNSW: Multi-layer graph (Navigable small-world graph)\n",
    "- RP-Trees: Binary trees with random projections\n",
    "- PQ: Codebooks and compressed codes\n",
    "- LSH: Hash tables with locality-sensitive hashing functions\n",
    "- KD-Trees: Forest of randomized KD-Trees\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B-Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [`README.md`](./README.md) and [`../README.md`](../README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider with have the following set of documents:\n",
    "\n",
    "```\n",
    "[\n",
    "  { \"id\": 1, \"name\": \"Product A\", \"price\": 10 },\n",
    "  { \"id\": 2, \"name\": \"Product B\", \"price\": 20 },\n",
    "  { \"id\": 3, \"name\": \"Product C\", \"price\": 15 },\n",
    "  { \"id\": 4, \"name\": \"Product D\", \"price\": 25 },\n",
    "  { \"id\": 5, \"name\": \"Product E\", \"price\": 30 }\n",
    "]\n",
    "```\n",
    "\n",
    "We want to build a B-Tree for them to be able to perform range-based search and filtering.\n",
    "We want to store the document ids in the nodes, but all tree operations are done based on the price value of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTreeNode:\n",
    "    def __init__(self, t, leaf=False):\n",
    "        self.t = t  # Minimum degree (defines the range for number of keys)\n",
    "        self.leaf = leaf\n",
    "        self.keys = []  # Array of keys\n",
    "        self.values = []  # Array of values (document IDs)\n",
    "        self.children = []  # Array of child pointers\n",
    "\n",
    "    def insert_non_full(self, key, value):\n",
    "        i = len(self.keys) - 1\n",
    "\n",
    "        if self.leaf:\n",
    "            self.keys.append(0)\n",
    "            self.values.append(0)\n",
    "            while i >= 0 and self.keys[i] > key:\n",
    "                self.keys[i + 1] = self.keys[i]\n",
    "                self.values[i + 1] = self.values[i]\n",
    "                i -= 1\n",
    "            self.keys[i + 1] = key\n",
    "            self.values[i + 1] = value\n",
    "        else:\n",
    "            while i >= 0 and self.keys[i] > key:\n",
    "                i -= 1\n",
    "            if len(self.children[i + 1].keys) == 2 * self.t - 1:\n",
    "                self.split_child(i + 1, self.children[i + 1])\n",
    "                if self.keys[i + 1] < key:\n",
    "                    i += 1\n",
    "            self.children[i + 1].insert_non_full(key, value)\n",
    "\n",
    "    def split_child(self, i, y):\n",
    "        t = self.t\n",
    "        z = BTreeNode(t, y.leaf)\n",
    "        self.children.insert(i + 1, z)\n",
    "        self.keys.insert(i, y.keys[t - 1])\n",
    "        self.values.insert(i, y.values[t - 1])\n",
    "        z.keys = y.keys[t:(2 * t - 1)]\n",
    "        z.values = y.values[t:(2 * t - 1)]\n",
    "        y.keys = y.keys[0:(t - 1)]\n",
    "        y.values = y.values[0:(t - 1)]\n",
    "\n",
    "        if not y.leaf:\n",
    "            z.children = y.children[t:(2 * t)]\n",
    "            y.children = y.children[0:t]\n",
    "\n",
    "class BTree:\n",
    "    def __init__(self, t):\n",
    "        self.root = BTreeNode(t, True)\n",
    "        self.t = t\n",
    "\n",
    "    def insert(self, key, value):\n",
    "        root = self.root\n",
    "        if len(root.keys) == 2 * self.t - 1:\n",
    "            temp = BTreeNode(self.t, False)\n",
    "            temp.children.insert(0, root)\n",
    "            temp.split_child(0, root)\n",
    "            i = 0\n",
    "            if temp.keys[0] < key:\n",
    "                i += 1\n",
    "            temp.children[i].insert_non_full(key, value)\n",
    "            self.root = temp\n",
    "        else:\n",
    "            root.insert_non_full(key, value)\n",
    "\n",
    "    def print_tree(self, node, lvl=0):\n",
    "        print(\"Level\", lvl, \":\", list(zip(node.keys, node.values)))\n",
    "        lvl += 1\n",
    "        if len(node.children) > 0:\n",
    "            for child in node.children:\n",
    "                self.print_tree(child, lvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0 : [(15, 3)]\n",
      "Level 1 : [(10, 1)]\n",
      "Level 1 : [(20, 2), (25, 4), (30, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "btree = BTree(2)  # B-tree with minimum degree 2\n",
    "documents = [\n",
    "    {\"id\": 1, \"price\": 10},\n",
    "    {\"id\": 2, \"price\": 20},\n",
    "    {\"id\": 3, \"price\": 15},\n",
    "    {\"id\": 4, \"price\": 25},\n",
    "    {\"id\": 5, \"price\": 30}\n",
    "]\n",
    "\n",
    "for doc in documents:\n",
    "    btree.insert(doc[\"price\"], doc[\"id\"])\n",
    "\n",
    "btree.print_tree(btree.root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [`README.md`](./README.md) and [`../README.md`](../README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider with have the following set of documents (same as before):\n",
    "\n",
    "```\n",
    "[\n",
    "  { \"id\": 1, \"name\": \"Product A\", \"price\": 10 },\n",
    "  { \"id\": 2, \"name\": \"Product B\", \"price\": 20 },\n",
    "  { \"id\": 3, \"name\": \"Product C\", \"price\": 15 },\n",
    "  { \"id\": 4, \"name\": \"Product D\", \"price\": 25 },\n",
    "  { \"id\": 5, \"name\": \"Product E\", \"price\": 30 }\n",
    "]\n",
    "```\n",
    "\n",
    "We want to build an inverted index for the field `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  { \"id\": 1, \"name\": \"Product A\", \"price\": 10 },\n",
    "  { \"id\": 2, \"name\": \"Product B\", \"price\": 20 },\n",
    "  { \"id\": 3, \"name\": \"Product C\", \"price\": 15 },\n",
    "  { \"id\": 4, \"name\": \"Product D\", \"price\": 25 },\n",
    "  { \"id\": 5, \"name\": \"Product E\", \"price\": 30 }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, ['Product', 'A']), (2, ['Product', 'B']), (3, ['Product', 'C']), (4, ['Product', 'D']), (5, ['Product', 'E'])]\n"
     ]
    }
   ],
   "source": [
    "### --- Tokenization\n",
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = doc['name'].split()\n",
    "    tokenized_docs.append((doc['id'], tokens))\n",
    "\n",
    "print(tokenized_docs)\n",
    "# Output: [(1, ['Product', 'A']), (2, ['Product', 'B']), (3, ['Product', 'C', 'Product', 'A'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: defaultdict(<class 'int'>, {'product': 1, 'a': 1}), 2: defaultdict(<class 'int'>, {'product': 1, 'b': 1}), 3: defaultdict(<class 'int'>, {'product': 1, 'c': 1}), 4: defaultdict(<class 'int'>, {'product': 1, 'd': 1}), 5: defaultdict(<class 'int'>, {'product': 1, 'e': 1})}\n"
     ]
    }
   ],
   "source": [
    "### -- Compute Term Frequencies (TF)\n",
    "from collections import defaultdict\n",
    "\n",
    "tf = defaultdict(lambda: defaultdict(int))\n",
    "for doc_id, tokens in tokenized_docs:\n",
    "    for token in tokens:\n",
    "        tf[doc_id][token.lower()] += 1\n",
    "\n",
    "print(dict(tf))\n",
    "# Output: {1: {'product': 1, 'a': 1}, 2: {'product': 1, 'b': 1}, 3: {'product': 2, 'c': 1, 'a': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product': 0.0, 'a': 1.6094379124341003, 'b': 1.6094379124341003, 'c': 1.6094379124341003, 'd': 1.6094379124341003, 'e': 1.6094379124341003}\n"
     ]
    }
   ],
   "source": [
    "### -- Compute Inverse Document Frequencies (IDF)\n",
    "import math\n",
    "\n",
    "df = defaultdict(int)\n",
    "for doc_id, tokens in tokenized_docs:\n",
    "    for token in set(tokens):\n",
    "        df[token.lower()] += 1\n",
    "\n",
    "idf = {term: math.log(len(documents) / df[term]) for term in df}\n",
    "\n",
    "print(idf)\n",
    "# Output: {'product': 0.0, 'a': 0.4054651081081644, 'b': 1.0986122886681098, 'c': 1.0986122886681098}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: defaultdict(<class 'float'>, {'product': 0.0, 'a': 1.6094379124341003}), 2: defaultdict(<class 'float'>, {'product': 0.0, 'b': 1.6094379124341003}), 3: defaultdict(<class 'float'>, {'product': 0.0, 'c': 1.6094379124341003}), 4: defaultdict(<class 'float'>, {'product': 0.0, 'd': 1.6094379124341003}), 5: defaultdict(<class 'float'>, {'product': 0.0, 'e': 1.6094379124341003})}\n"
     ]
    }
   ],
   "source": [
    "### -- Compute TF-IDF\n",
    "tf_idf = defaultdict(lambda: defaultdict(float))\n",
    "for doc_id, tokens in tokenized_docs:\n",
    "    for token in tokens:\n",
    "        tf_idf[doc_id][token.lower()] = tf[doc_id][token.lower()] * idf[token.lower()]\n",
    "\n",
    "print(dict(tf_idf))\n",
    "# Output: {1: {'product': 0.0, 'a': 0.4054651081081644}, 2: {'product': 0.0, 'b': 1.0986122886681098}, 3: {'product': 0.0, 'c': 1.0986122886681098, 'a': 0.4054651081081644}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product': [(1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0)], 'a': [(1, 1.6094379124341003)], 'b': [(2, 1.6094379124341003)], 'c': [(3, 1.6094379124341003)], 'd': [(4, 1.6094379124341003)], 'e': [(5, 1.6094379124341003)]}\n"
     ]
    }
   ],
   "source": [
    "### -- Build the Inverted Index\n",
    "inverted_index = defaultdict(list)\n",
    "for doc_id, scores in tf_idf.items():\n",
    "    for term, score in scores.items():\n",
    "        inverted_index[term].append((doc_id, score))\n",
    "\n",
    "print(dict(inverted_index))\n",
    "# Output: {'product': [(1, 0.0), (2, 0.0), (3, 0.0)], 'a': [(1, 0.4054651081081644), (3, 0.4054651081081644)], 'b': [(2, 1.0986122886681098)], 'c': [(3, 1.0986122886681098)]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc Values, Columnar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [`README.md`](./README.md) and [`../README.md`](../README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider with have the following set of documents (same as before):\n",
    "\n",
    "```\n",
    "[\n",
    "  { \"id\": 1, \"name\": \"Product A\", \"price\": 10 },\n",
    "  { \"id\": 2, \"name\": \"Product B\", \"price\": 20 },\n",
    "  { \"id\": 3, \"name\": \"Product C\", \"price\": 15 },\n",
    "  { \"id\": 4, \"name\": \"Product D\", \"price\": 25 },\n",
    "  { \"id\": 5, \"name\": \"Product E\", \"price\": 30 }\n",
    "]\n",
    "```\n",
    "\n",
    "We want to build a Doc Value or columnar representation of `price` to be able to compute fast field aggragation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  { \"id\": 1, \"name\": \"Product A\", \"price\": 10 },\n",
    "  { \"id\": 2, \"name\": \"Product B\", \"price\": 20 },\n",
    "  { \"id\": 3, \"name\": \"Product C\", \"price\": 15 },\n",
    "  { \"id\": 4, \"name\": \"Product D\", \"price\": 25 },\n",
    "  { \"id\": 5, \"name\": \"Product E\", \"price\": 30 }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize numpy array for prices\n",
    "prices = np.zeros(len(documents))\n",
    "\n",
    "# Initialize bitstring\n",
    "bitstring = np.zeros(len(documents), dtype=int)\n",
    "\n",
    "# Initialize dictionary to map document IDs to array indices\n",
    "doc_id_to_index = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices Array: [10. 20. 15. 25. 30.]\n",
      "Bitstring: [1 1 1 1 1]\n",
      "Document ID to Index Mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n"
     ]
    }
   ],
   "source": [
    "for idx, doc in enumerate(documents):\n",
    "    prices[idx] = doc[\"price\"]\n",
    "    bitstring[idx] = 1  # 1 indicates the presence of the document\n",
    "    doc_id_to_index[doc[\"id\"]] = idx\n",
    "\n",
    "print(\"Prices Array:\", prices)\n",
    "print(\"Bitstring:\", bitstring)\n",
    "print(\"Document ID to Index Mapping:\", doc_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Bitstring: [0 1 0 ... 1 1 1]\n",
      "Mean Price: 49.99163888795988\n"
     ]
    }
   ],
   "source": [
    "# Switch off documents with IDs 1 and 3\n",
    "bitstring[doc_id_to_index[1]] = 0\n",
    "bitstring[doc_id_to_index[3]] = 0\n",
    "\n",
    "print(\"Updated Bitstring:\", bitstring)\n",
    "\n",
    "# Compute the mean price using masked array\n",
    "mean_price = np.ma.masked_array(prices, mask=bitstring == 0).mean()\n",
    "\n",
    "# Alternative\n",
    "# masked_prices = prices[bitstring == 1]\n",
    "# mean_price = masked_prices.mean()\n",
    "# Which option is faster and more efficient?\n",
    "# The new array will require more memory,\n",
    "# the speed is benchmarked below...\n",
    "\n",
    "print(\"Mean Price:\", mean_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking of Masked vs. New Array Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New array mean price: 50.00747511111111, Time: 0.057581424713134766 seconds\n",
      "Masked array mean price: 50.00747511111111, Time: 0.04833984375 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Large example dataset\n",
    "num_docs = 10_000_000\n",
    "documents = [{\"id\": i, \"price\": np.random.randint(1, 100)} for i in range(num_docs)]\n",
    "\n",
    "# Initialize numpy array for prices\n",
    "prices = np.zeros(num_docs)\n",
    "\n",
    "# Initialize bitstring\n",
    "bitstring = np.ones(num_docs, dtype=int)\n",
    "\n",
    "# Populate the arrays and mappings\n",
    "for idx, doc in enumerate(documents):\n",
    "    prices[idx] = doc[\"price\"]\n",
    "\n",
    "# Switch off some documents\n",
    "bitstring[::10] = 0  # Switch off every 10th document\n",
    "\n",
    "# Benchmark creating a new array\n",
    "start_time = time.time()\n",
    "masked_prices = prices[bitstring == 1]\n",
    "mean_price_new_array = masked_prices.mean()\n",
    "new_array_time = time.time() - start_time\n",
    "\n",
    "# Benchmark applying a mask\n",
    "start_time = time.time()\n",
    "mean_price_mask = np.ma.masked_array(prices, mask=bitstring == 0).mean()\n",
    "mask_time = time.time() - start_time\n",
    "\n",
    "print(f\"New array mean price: {mean_price_new_array}, Time: {new_array_time} seconds\")\n",
    "print(f\"Masked array mean price: {mean_price_mask}, Time: {mask_time} seconds\")\n",
    "# The new array is almost 2x faster than the masked array approach\n",
    "# but that's maybe not always the case..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KD-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A KD-tree (k-dimensional tree) is a good data structure for certain types of search operations in a vectorized space, particularly for low-dimensional spaces. However, its effectiveness can decrease as the dimensionality of the data increases.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Efficient for Low Dimensions: KD-trees are very efficient for searching, nearest neighbor queries, and range searches in low-dimensional spaces (typically up to 10-20 dimensions).\n",
    "- Balanced Tree Structure: KD-trees provide balanced partitioning of the space, which helps in maintaining good search performance.\n",
    "- Recursive Space Partitioning: They recursively partition the space into axis-aligned hyper-rectangles, allowing for efficient search operations within these partitions.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Curse of Dimensionality: As the number of dimensions increases, the performance of KD-trees degrades significantly. This is because the number of points within a given distance of a query point grows exponentially with the number of dimensions, making the partitioning less effective.\n",
    "- High Memory Usage: In high-dimensional spaces, the tree can become very large and memory-intensive.\n",
    "- Inefficient for High Dimensions: For high-dimensional spaces, KD-trees may need to examine a large fraction of the points, making them inefficient compared to other data structures specifically designed for high-dimensional data.\n",
    "\n",
    "Alternatives which are more efficient:\n",
    "\n",
    "- Hierarchical Navigable Small World Graphs (HNSW): Highly efficient and scalable for high-dimensional spaces.\n",
    "    - `hnswlib`: Implements HNSW for fast approximate nearest neighbor search.\n",
    "- Locality-Sensitive Hashing (LSH): Uses hash functions to partition data and perform efficient approximate nearest neighbor searches.\n",
    "- PQ and OPQ (Optimized Product Quantization): Compresses high-dimensional vectors into smaller codes, allowing efficient approximate nearest neighbor search in compressed space.\n",
    "- FAISS (Facebook AI Similarity Search): Provides various algorithms optimized for high-dimensional vector search, including IVF, PQ, and HNSW.\n",
    "- Annoy (Approximate Nearest Neighbors Oh Yeah): Uses random projection trees for efficient high-dimensional nearest neighbor search.\n",
    "\n",
    "See [`README.md`](./README.md) and [`../README.md`](../README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KDTreeNode:\n",
    "    def __init__(self, point, left=None, right=None):\n",
    "        self.point = point\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class KDTree:\n",
    "    def __init__(self, points):\n",
    "        self.root = self.build_tree(points)\n",
    "    \n",
    "    def build_tree(self, points, depth=0):\n",
    "        if len(points) == 0:\n",
    "            return None\n",
    "\n",
    "        k = len(points[0])  # dimension of the points\n",
    "        axis = depth % k\n",
    "\n",
    "        # Sort points and choose the median as the pivot element\n",
    "        points.sort(key=lambda x: x[axis])\n",
    "        median = len(points) // 2\n",
    "\n",
    "        # Create node and construct subtrees\n",
    "        return KDTreeNode(\n",
    "            point=points[median],\n",
    "            left=self.build_tree(points[:median], depth + 1),\n",
    "            right=self.build_tree(points[median + 1:], depth + 1)\n",
    "        )\n",
    "    \n",
    "    def nearest_neighbor(self, query_point, return_distance=False):\n",
    "        best = [None, float('inf')]  # [best_point, best_distance]\n",
    "        self._nearest(self.root, query_point, 0, best)\n",
    "        return (best[0], best[1]) if return_distance else best[0]\n",
    "    \n",
    "    def _nearest(self, node, query_point, depth, best):\n",
    "        if node is None:\n",
    "            return\n",
    "\n",
    "        k = len(query_point)\n",
    "        axis = depth % k\n",
    "\n",
    "        # Compute the Euclidean distance between the query point and the current node's point\n",
    "        distance = np.linalg.norm(np.array(query_point) - np.array(node.point))\n",
    "\n",
    "        if distance < best[1]:\n",
    "            best[0] = node.point\n",
    "            best[1] = distance\n",
    "\n",
    "        # Determine which subtree to search first\n",
    "        diff = query_point[axis] - node.point[axis]\n",
    "        close, away = (node.left, node.right) if diff < 0 else (node.right, node.left)\n",
    "\n",
    "        # Search the close subtree\n",
    "        self._nearest(close, query_point, depth + 1, best)\n",
    "\n",
    "        # If the distance to the best point found so far is greater than the distance to the splitting plane,\n",
    "        # we must search the away subtree as well\n",
    "        if abs(diff) < best[1]:\n",
    "            self._nearest(away, query_point, depth + 1, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random points in 10-dimensional space\n",
    "points = np.random.rand(100, 10).tolist()\n",
    "\n",
    "# Query point\n",
    "query_point = np.random.rand(10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbor: [0.4543671214950068, 0.46915778666863317, 0.34874708320227554, 0.5158381589045584, 0.019949076725799042, 0.8928449300859492, 0.6717497206842002, 0.3861436314301726, 0.7602567876061882, 0.1377233498132474]\n",
      "Distance: 0.694748771378379\n"
     ]
    }
   ],
   "source": [
    "# Build KD-tree\n",
    "kdtree = KDTree(points)\n",
    "\n",
    "# Find the nearest neighbor using the KD-tree\n",
    "nearest = kdtree.nearest_neighbor(query_point, return_distance=True)\n",
    "print(\"Nearest neighbor:\", nearest[0])\n",
    "print(\"Distance:\", nearest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbor: [0.4543671214950068, 0.46915778666863317, 0.34874708320227554, 0.5158381589045584, 0.019949076725799042, 0.8928449300859492, 0.6717497206842002, 0.3861436314301726, 0.7602567876061882, 0.1377233498132474]\n",
      "Distance: 0.694748771378379\n"
     ]
    }
   ],
   "source": [
    "# Brute force: compute distance against all points\n",
    "def brute_force_nn(query_point, points):\n",
    "    distances = np.linalg.norm(np.array(query_point) - np.array(points), axis=1)\n",
    "    p = distances.argmin()\n",
    "    return points[p], distances[p]\n",
    "\n",
    "# Find the nearest neighbor using brute force\n",
    "nearest = brute_force_nn(query_point, points)\n",
    "\n",
    "print(f\"Nearest neighbor: {nearest[0]}\")\n",
    "print(f\"Distance: {nearest[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark the KD-tree approach\n",
    "def benchmark(dim, num_points, num_experiments):\n",
    "    kd_times = []\n",
    "    bf_times = []\n",
    "    kd_creation_times = []\n",
    "\n",
    "    for _ in range(num_experiments):\n",
    "        # Generate random points\n",
    "        points = np.random.rand(num_points, dim).tolist()\n",
    "        query_point = np.random.rand(dim).tolist()\n",
    "\n",
    "        # Measure KD-tree creation time\n",
    "        start_time = time.time()\n",
    "        kdtree = KDTree(points)\n",
    "        kd_creation_times.append(time.time() - start_time)\n",
    "\n",
    "        # Measure KD-tree nearest neighbor search time\n",
    "        start_time = time.time()\n",
    "        kd_nearest = kdtree.nearest_neighbor(query_point, return_distance=True)\n",
    "        kd_times.append(time.time() - start_time)\n",
    "\n",
    "        # Measure brute-force nearest neighbor search time\n",
    "        start_time = time.time()\n",
    "        bf_nearest = brute_force_nn(query_point, points)\n",
    "        bf_times.append(time.time() - start_time)\n",
    "\n",
    "    # Compute overall statistics\n",
    "    kd_creation_mean = np.mean(kd_creation_times)\n",
    "    kd_mean = np.mean(kd_times)\n",
    "    kd_std = np.std(kd_times)\n",
    "    bf_mean = np.mean(bf_times)\n",
    "    bf_std = np.std(bf_times)\n",
    "\n",
    "    return {\n",
    "        \"kd_creation_time_mean\": kd_creation_mean,\n",
    "        \"kd_search_time_mean\": kd_mean,\n",
    "        \"kd_search_time_std\": kd_std,\n",
    "        \"brute_force_time_mean\": bf_mean,\n",
    "        \"brute_force_time_std\": bf_std\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD-tree creation time (mean): 1.9230053901672364\n",
      "KD-tree search time (mean): 0.019190096855163576\n",
      "KD-tree search time (std): 0.015693908307997842\n",
      "Brute-force search time (mean): 0.14356534481048583\n",
      "Brute-force search time (std): 0.015259770504133253\n"
     ]
    }
   ],
   "source": [
    "dim = 10\n",
    "num_points = 100_000\n",
    "num_experiments = 10\n",
    "\n",
    "stats = benchmark(dim, num_points, num_experiments)\n",
    "print(\"KD-tree creation time (mean):\", stats[\"kd_creation_time_mean\"])\n",
    "print(\"KD-tree search time (mean):\", stats[\"kd_search_time_mean\"])\n",
    "print(\"KD-tree search time (std):\", stats[\"kd_search_time_std\"])\n",
    "print(\"Brute-force search time (mean):\", stats[\"brute_force_time_mean\"])\n",
    "print(\"Brute-force search time (std):\", stats[\"brute_force_time_std\"])\n",
    "# with \n",
    "# dim = 10\n",
    "# num_points = 100_000\n",
    "# num_experiments = 10\n",
    "# KD-tree search is 10x faster than the brute force with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
